{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script aims to run the regular re-accession for CKAN portals. Compared with DCAT portals, CKAN updates less frequently. Thus, we often run the script every 3 months.\n",
    "\n",
    "The old version of the script may harvest and write reports by portal. However, this script is able to run all portals and create final reports all at once.\n",
    "\n",
    "> Orignal created by Yijing Zhou (@YijingZhou33) and Ziying Cheng(@Ziiiiing)\n",
    "\n",
    "> Updated January 15, 2021                           \n",
    "> Updated by Ziying Cheng (@Ziiiiing)\n",
    "\n",
    "> Updated July 05, 2021                           \n",
    "> Updated by Ziying Cheng (@Ziiiiing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check files before execute the following code. In the directory, you will have the following files:\n",
    "\n",
    "- `harvest.ipynb`\n",
    "- `CKANportals.csv` includes some basic information about each CKAN portal.\n",
    "- `resource` folder collects existing resource names by portal for each re-accession. The new one will be compared with the latest one to get both the created and deleted datasets.\n",
    "- `reports` folder stores the metadata CSV files for all **created** dataset which are named as `allNewItems_YYYYMMDD.csv`. Besides, **deleted** datasets are also stored within CSV files called `allDeletedItems_YYYYMMDD.csv`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pip install <module_name>` to install a missing python module if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import urllib.request\n",
    "import json \n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from html.parser import HTMLParser\n",
    "import re\n",
    "import ast\n",
    "import decimal\n",
    "import ssl\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-generate the current time in 'YYYYMM' format\n",
    "actionDate = time.strftime('202212')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract portal information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from local `CKANportals.csv` ane extract the `URL`, `Provider`, `Publisher`, `Spatial Coverage` and `Bounding box` for each `portalName`."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 21,
=======
   "execution_count": 4,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "portalsInfo = {}\n",
    "\n",
    "with open('CKANportals.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    \n",
    "    # jump over the fieldnames\n",
    "    # loop over from the first content record\n",
    "    csv_fields = next(reader)\n",
    "    for row in reader:\n",
    "        portalsInfo[row[0]] = [row[1], row[2], row[3], row[4], row[5]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Loop over portals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over each portal, collect the up-to-date resources and compare with the latest resources list from the `resource` folder. Thus, we can get the created datasets and deleted datasets after comparison. For those newly created datasets, request and create their metadata. For those deleted, store the resource name along with its portal code in the CSV file."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 22,
=======
   "execution_count": 5,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compare old and new resource list\n",
    "# return created and deleted items separately\n",
    "\n",
    "def returnNotMatches(old, new):\n",
    "    oldResource = set(old)\n",
    "    newResource = set(new)\n",
    "    return [list(newResource - oldResource), list(oldResource - newResource)]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 23,
=======
   "execution_count": 6,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to removes html tags from text\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True        \n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def cleanData(value):\n",
    "    fieldvalue = strip_tags(value)\n",
    "    return fieldvalue"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 24,
=======
   "execution_count": 7,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to format metadata for new items\n",
    "\n",
    "def metadataNewItems(newdata):    \n",
    "    metadata = []\n",
    "    \n",
    "    title = ''\n",
    "    alternativeTitle = newdata['result']['title']\n",
    "        \n",
    "    description = cleanData(newdata['result']['notes'])\n",
    "    ### Remove newline, whitespace, defalut description and replace singe quote, double quote \n",
    "    if description == '{{default.description}}':\n",
    "        description = description.replace('{{default.description}}', '')\n",
    "    else:\n",
    "        description = re.sub(r'[\\n]+|[\\r\\n]+',' ', description, flags=re.S)\n",
    "        description = re.sub(r'\\s{2,}' , ' ', description)\n",
    "        description = description.replace(u'\\u2019', \"'\").replace(u'\\u201c', '\\\"').replace(u'\\u201d', '\\\"').replace(u'\\u00a0', '').replace(u'\\u00b7', '').replace(u'\\u2022', '').replace(u'\\u2013','-').replace(u'\\u200b', '')\n",
    "\n",
    "    language = 'eng'  \n",
    "    creator = ''\n",
    "    index = 0\n",
    "        \n",
    "    publisher = portalPublisher       \n",
    "    spatialCoverage = portalSpaCov   \n",
    "\n",
    "    if 'extras' in newitem['result']:\n",
    "        extras = newitem['result']['extras']    \n",
    "        for dictionary in extras:\n",
    "            if dictionary['key'] == 'dsOriginator':\n",
    "                creator = dictionary['value']\n",
    "\n",
    "                ## if Creator field contains keywork 'County', extract the county name to fill in Publisher and Spatial Coverage field\n",
    "                ## otherwise, autofill both fileds with 'Minnesota'\n",
    "                index = creator.find('County')\n",
    "                if index != -1:\n",
    "                    publisher = creator[: index + 6]\n",
    "                    spatialCoverage = publisher + f', {portalSpaCov}|{portalSpaCov}'   \n",
    "    \n",
    "                            \n",
    "    format_types = []\n",
    "    resourceClass = ''\n",
    "    formatElement = ''\n",
    "    downloadURL =  ''\n",
    "    resourceType = ''\n",
    "    featureServer = ''\n",
    "    webService = ''\n",
    "    html = ''\n",
    "    previewImg = ''\n",
    "    \n",
    "    distribution = newdata['result']['resources']\n",
    "    for dictionary in distribution:\n",
    "        try:\n",
    "            ### if one of the distributions is a shapefile, change genre/format and get the downloadURL\n",
    "            format_types.extend([dictionary['format']])\n",
    "            if dictionary['format'] == 'SHP':\n",
    "                resourceClass = 'Datasets'\n",
    "                formatElement = 'Shapefile'\n",
    "                downloadURL = dictionary['url']\n",
    "                resourceType = 'Vector data'\n",
    "                \n",
    "                \n",
    "            ### if one of the distributions is WMS, and it is taged as 'aerial photography'\n",
    "            ### change genre, type, and format to relate to imagery\n",
    "            if dictionary['format'] == 'WMS':\n",
    "                tags = newdata['result']['tags']\n",
    "                for tag in tags:\n",
    "                    if tag['display_name'] == 'aerial photography':                        \n",
    "                        resourceClass = 'Imagery'\n",
    "                        formatElement = 'Imagery'\n",
    "                        downloadURL = dictionary['url']\n",
    "                        resourceType = 'Satellite imagery'\n",
    "                        \n",
    "            ### saves the url if the dataset has Webservice format         \n",
    "            if dictionary['format'] == 'ags_mapserver':\n",
    "                webService = dictionary['url']\n",
    "                \n",
    "            ### saves the metadata page\n",
    "            if dictionary['format'] == 'HTML':\n",
    "                html = dictionary['url']   \n",
    "            \n",
    "            ### saves the thumbnail iamge\n",
    "            if dictionary['format'] == 'JPEG':\n",
    "                previewImg = dictionary['url']    \n",
    "                \n",
    "        ### if the distribution section of the metadata is not structured in a typical way\n",
    "        except:\n",
    "            resourceClass = ''\n",
    "            formatElement = ''\n",
    "            downloadURL =  ''       \n",
    "            continue\n",
    "                                                \n",
    "    \n",
    "    ### extracts the bounding box \n",
    "    try:\n",
    "        bbox = []\n",
    "        spatial = ''\n",
    "        extra_spatial = newdata['result']['extras']\n",
    "        for dictionary in extra_spatial:\n",
    "            if dictionary['key'] == 'spatial':\n",
    "                spatialList = ast.literal_eval(dictionary['value'].split(':[')[1].split(']}')[0])\n",
    "                coordmin = spatialList[0]\n",
    "                coordmax = spatialList[2]\n",
    "                coordmin.extend(coordmax)\n",
    "                typeDmal = decimal.Decimal\n",
    "                fix3 = typeDmal(\"0.001\")\n",
    "                for coord in coordmin:\n",
    "                    coordFix = typeDmal(coord).quantize(fix3)\n",
    "                    bbox.extend([str(coordFix)])\n",
    "                    spatial = ','.join(bbox)            \n",
    "    except:\n",
    "        spatial = ''     \n",
    "        \n",
    "    try:\n",
    "        theme = ''\n",
    "        groups_theme = newdata['result']['groups']\n",
    "        if len(groups_theme) != 0:\n",
    "            theme = groups_theme[0]['display_name'].replace('+', 'and')\n",
    "    except:\n",
    "        theme = ''\n",
    "    \n",
    "    keyword_list = []\n",
    "    keyword = newdata['result']['tags']\n",
    "    for dictionary in keyword:\n",
    "        keyword_list.extend([dictionary['display_name']])\n",
    "    keyword_list = ','.join(keyword_list).replace(',', '|')\n",
    "    \n",
    "    dateIssued = newdata['result']['metadata_created']\n",
    "    temporalCoverage = 'Continually updated resource'\n",
    "    dateRange = ''\n",
    "    \n",
    "    information = landingurl + newdata['result']['name']\n",
    "    ID = newdata['result']['id']\n",
    "    \n",
    "    featureServer = ''\n",
    "    mapServer = ''\n",
    "    imageServer = ''\n",
    "    \n",
    "    ### specifies the Webservice type by querying the webService string    \n",
    "    try:\n",
    "        if 'FeatureServer' in webService:\n",
    "            featureServer = webService\n",
    "        if 'MapServer' in webService:\n",
    "            mapServer = webService\n",
    "        if 'ImageServer' in webService:\n",
    "            imageServer = webService\n",
    "    except:\n",
    "            print(ID)\n",
    "    \n",
    "    identifier = item\n",
    "    provider = portalProvider  \n",
    "    code = portal     \n",
    "    memberOf = 'ba5cc745-21c5-4ae9-954b-72dd8db6815a'\n",
    "    isPartOf = portal\n",
    "    \n",
    "    \n",
    "    status = 'Active'\n",
    "    accuralMethod = 'CKAN'\n",
    "    dateAccessioned = time.strftime('%Y-%m-%d')\n",
    "                \n",
    "    rights = ''               \n",
    "    accessRights = 'Public'\n",
    "    suppressed = 'FALSE'\n",
    "    childRecord = 'FALSE'\n",
    "    \n",
    "    metadataList = [title, alternativeTitle, description, language, creator, publisher,\n",
    "                    resourceClass, theme, keyword_list, dateIssued, temporalCoverage,\n",
    "                    dateRange, spatialCoverage, spatial, resourceType,\n",
    "                    formatElement, information, downloadURL, mapServer, featureServer,\n",
    "                    imageServer, html, previewImg, ID, identifier, provider, code, memberOf, isPartOf, status,\n",
    "                    accuralMethod, dateAccessioned, rights, accessRights, suppressed, childRecord]\n",
    "    \n",
    "    ### check the resource class: if it is neither 'Datasets' nor 'Imagery', create a empty list\n",
    "    for i in range(len(metadataList)):\n",
    "        if metadataList[6] != '':\n",
    "            metadata = metadataList\n",
    "        else: \n",
    "            continue\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 25,
=======
   "execution_count": 8,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Harvesting portal 05a-01\n",
      ">>> Collecting dataset(1/8): https://gisdata.mn.gov/api/3/action/package_show?id=env-forest-health-survey-2022\n",
<<<<<<< Updated upstream
      ">>> Collecting dataset(2/8): https://gisdata.mn.gov/api/3/action/package_show?id=us-mn-co-dakota-base-aerialphotography-2011-lfon\n",
      ">>> Collecting dataset(3/8): https://gisdata.mn.gov/api/3/action/package_show?id=us-mn-co-dakota-base-aerialphotography-2008-sprg\n",
      ">>> Collecting dataset(4/8): https://gisdata.mn.gov/api/3/action/package_show?id=us-mn-co-dakota-base-aerialphotography-2022-sprg\n",
      ">>> Collecting dataset(5/8): https://gisdata.mn.gov/api/3/action/package_show?id=water-drinking-water-supply-sw\n",
      ">>> Collecting dataset(6/8): https://gisdata.mn.gov/api/3/action/package_show?id=geos-mtsimon-hinckley-aquifer\n",
      ">>> Collecting dataset(7/8): https://gisdata.mn.gov/api/3/action/package_show?id=water-groundwater-capture-zones\n",
      ">>> Collecting dataset(8/8): https://gisdata.mn.gov/api/3/action/package_show?id=us-mn-co-dakota-base-aerialphotography-2009-lfon\n",
      "\n",
      "Harvesting portal 08c-01\n",
      ">>> Collecting dataset(1/7): https://www.opendataphilly.org/api/3/action/package_show?id=tobacco-retailer-density-caps\n",
      ">>> Collecting dataset(2/7): https://www.opendataphilly.org/api/3/action/package_show?id=connecting-philadelphia-report\n",
      ">>> Collecting dataset(3/7): https://www.opendataphilly.org/api/3/action/package_show?id=philadelphia-household-internet-assessment-survey\n",
      ">>> Collecting dataset(4/7): https://www.opendataphilly.org/api/3/action/package_show?id=blogpost-on-language-usage-dashboard\n",
      ">>> Collecting dataset(5/7): https://www.opendataphilly.org/api/3/action/package_show?id=language-usage-dashboard\n",
      ">>> Collecting dataset(6/7): https://www.opendataphilly.org/api/3/action/package_show?id=phl-language-services-usage\n",
      ">>> Collecting dataset(7/7): https://www.opendataphilly.org/api/3/action/package_show?id=pwd-geotechnical-tests\n",
=======
      ">>> Collecting dataset(2/8): https://gisdata.mn.gov/api/3/action/package_show?id=us-mn-co-dakota-base-aerialphotography-2008-sprg\n",
      ">>> Collecting dataset(3/8): https://gisdata.mn.gov/api/3/action/package_show?id=us-mn-co-dakota-base-aerialphotography-2009-lfon\n",
      ">>> Collecting dataset(4/8): https://gisdata.mn.gov/api/3/action/package_show?id=us-mn-co-dakota-base-aerialphotography-2022-sprg\n",
      ">>> Collecting dataset(5/8): https://gisdata.mn.gov/api/3/action/package_show?id=water-groundwater-capture-zones\n",
      ">>> Collecting dataset(6/8): https://gisdata.mn.gov/api/3/action/package_show?id=us-mn-co-dakota-base-aerialphotography-2011-lfon\n",
      ">>> Collecting dataset(7/8): https://gisdata.mn.gov/api/3/action/package_show?id=geos-mtsimon-hinckley-aquifer\n",
      ">>> Collecting dataset(8/8): https://gisdata.mn.gov/api/3/action/package_show?id=water-drinking-water-supply-sw\n",
      "\n",
      "Harvesting portal 08c-01\n",
      ">>> Collecting dataset(1/7): https://www.opendataphilly.org/api/3/action/package_show?id=connecting-philadelphia-report\n",
      ">>> Collecting dataset(2/7): https://www.opendataphilly.org/api/3/action/package_show?id=language-usage-dashboard\n",
      ">>> Collecting dataset(3/7): https://www.opendataphilly.org/api/3/action/package_show?id=blogpost-on-language-usage-dashboard\n",
      ">>> Collecting dataset(4/7): https://www.opendataphilly.org/api/3/action/package_show?id=phl-language-services-usage\n",
      ">>> Collecting dataset(5/7): https://www.opendataphilly.org/api/3/action/package_show?id=philadelphia-household-internet-assessment-survey\n",
      ">>> Collecting dataset(6/7): https://www.opendataphilly.org/api/3/action/package_show?id=pwd-geotechnical-tests\n",
      ">>> Collecting dataset(7/7): https://www.opendataphilly.org/api/3/action/package_show?id=tobacco-retailer-density-caps\n",
>>>>>>> Stashed changes
      "\n",
      "Harvesting portal 08f-01\n",
      ">>> Collecting dataset(1/1): http://data.wprdc.org/api/3/action/package_show?id=allegheny-county-census-blocks-2021\n",
      "\n",
      "Harvesting portal 01c-01\n",
      "\n",
<<<<<<< Updated upstream
      "Harvesting portal 02a-03\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'resource/02a-03_3_202212.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m oldResource \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresource/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mportal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moldDate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m oldList \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moldResource\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fr:\n\u001b[1;32m     55\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(fr)\n\u001b[1;32m     56\u001b[0m     field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(reader)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resource/02a-03_3_202212.csv'"
=======
      "Harvesting portal 02a-03\n",
      ">>> Collecting dataset(1/1): https://data.illinois.gov/api/3/action/package_show?id=grants-to-illinois-artists-and-arts-organizations-in-2022-3rd-quarterly-reports\n",
      "\n",
      "Harvesting portal 05d-11\n",
      ">>> skip 05d-11\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "AllNewMetadata = []\n",
    "AllDeleltedItem = []\n",
    "\n",
    "for portal in portalsInfo:     \n",
    "    print()\n",
    "    print(f'Harvesting portal {portal}')\n",
    "    \n",
    "    ### delete later\n",
    "    if portal == '05d-11':\n",
    "        print('>>> skip 05d-11')\n",
    "        continue\n",
    "\n",
    "    portalURL = portalsInfo[portal][0]\n",
    "    portalProvider = portalsInfo[portal][1]\n",
    "    portalPublisher = portalsInfo[portal][2]\n",
    "    portalSpaCov = portalsInfo[portal][3]\n",
    "\n",
    "    packageURL = portalURL + 'api/3/action/package_list'\n",
    "    landingurl = portalURL + 'dataset/'\n",
    "\n",
    "    # request new resources list\n",
    "    context = ssl._create_unverified_context()\n",
    "    response = urllib.request.urlopen(packageURL, context=context).read()\n",
    "    packageList = json.loads(response.decode('utf-8'))\n",
    "    newList = packageList['result']\n",
    "\n",
    "    # store new resources locally for next re-accession\n",
    "    with open(f'resource/{portal}_{actionDate}.csv', 'w') as fw:\n",
    "        writer = csv.writer(fw)\n",
    "        field = ['result']\n",
    "        rows = np.reshape(newList, (-1, 1))\n",
    "        writer.writerow(field)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    # find the latest resources list\n",
    "    dates = []\n",
    "    filenames = os.listdir('resource')\n",
    "    for filename in filenames:\n",
    "        if filename.startswith(portal):\n",
    "            dates.append(filename[-12:-4]) \n",
    "\n",
    "    if actionDate in dates:\n",
    "        dates.remove(actionDate)\n",
    "\n",
    "\n",
    "    # For portals already existed for last re-accession:\n",
    "    ## compare the current and the latest resources\n",
    "    ## and find new and deleted items\n",
    "    if dates:\n",
    "        oldDate = max(dates)\n",
    "        oldResource = f'resource/{portal}_{oldDate}.csv'\n",
    "\n",
    "        oldList = []\n",
    "        with open(oldResource) as fr:\n",
    "            reader = csv.reader(fr)\n",
    "            field = next(reader)\n",
    "            for row in reader:\n",
    "                oldList.append(row[0])\n",
    "\n",
    "        newItems = []\n",
    "        deletedItems = []\n",
    "\n",
    "        newItems = returnNotMatches(oldList, newList)[0]\n",
    "        deletedItems = returnNotMatches(oldList, newList)[1]\n",
    "        AllDeleltedItem += [[portal, x] for x in deletedItems]\n",
    "\n",
    "\n",
    "    # For new portals:\n",
    "    # all current resources are new and do not have deleted items\n",
    "    else:\n",
    "        newItems = newList\n",
    "\n",
    "\n",
    "    # Create metadata for all new items for each portal\n",
    "    withEmpty = []\n",
    "    metadata = []\n",
    "    count = 0\n",
    "    total = len(newItems)\n",
    "\n",
    "    for item in newItems:\n",
    "        count += 1\n",
    "        itemURL = portalURL + 'api/3/action/package_show?id=' + item\n",
    "        print(f'>>> Collecting dataset({count}/{total}): {itemURL}')\n",
    "\n",
    "        context = ssl._create_unverified_context()\n",
    "        response = urllib.request.urlopen(itemURL, context=context).read()\n",
    "        newitem = json.loads(response.decode('utf-8'))\n",
    "        withEmpty.append(metadataNewItems(newitem))\n",
    "\n",
    "    # check whether empty\n",
    "    metadata = [x for x in withEmpty if x != []]\n",
    "    AllNewMetadata += metadata "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Print Reports"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 26,
=======
   "execution_count": 9,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def printReport(report, fields, datalist):\n",
    "    with open(report, 'w', newline='', encoding='utf-8') as f:\n",
    "        csvout = csv.writer(f)\n",
    "        csvout.writerow(fields)\n",
    "        csvout.writerows(datalist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write CSV file for all new datasets."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 27,
=======
   "execution_count": 10,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames_new = ['Title', 'Alternative Title', 'Description', 'Language', 'Creator', 'titleSource', 'Resource Class',\n",
    "              'Theme', 'Keyword', 'Date Issued', 'Temporal Coverage', 'Date Range', 'Spatial Coverage',\n",
    "              'Bounding Box', 'Resource Type', 'Format', 'Information', 'Download', 'MapServer', \n",
    "              'FeatureServer', 'ImageServer', 'HTML', 'Image', 'ID', 'Identifier', 'Provider', 'Code', 'Member Of', \n",
    "              'Is Part Of', 'Status', 'Accrual Method', 'Date Accessioned', 'Rights', 'Access Rights', 'Suppressed', 'Child Record']\n",
    "\n",
    "filepath_new = f'reports/allNewItems_{actionDate}.csv'   \n",
    "printReport(filepath_new, fieldnames_new, AllNewMetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write CSV file for all deleted datasets."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 28,
=======
   "execution_count": 11,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames_del = ['Portal', 'Resource']\n",
    "\n",
    "filepath_del = f'reports/allDeletedItems_{actionDate}.csv'   \n",
    "printReport(filepath_del, fieldnames_del, AllDeleltedItem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Manual Edits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the metadata CSV file on Google Drive and continue with the manual edits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "version": "3.10.6"
=======
   "version": "3.9.12"
>>>>>>> Stashed changes
=======
   "version": "3.9.12"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
